{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9309eaed-d83d-4523-8dca-09f4dcb8f4fb",
   "metadata": {},
   "source": [
    "# Models, Prompts and Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4c0e271e-db29-4f52-ac96-8eca80a20d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c740402-8f25-4386-afa6-72306faa7545",
   "metadata": {},
   "source": [
    "## Direct API Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd98b6df-e3ec-4326-a636-47fc20b5a51b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### NVIDIA Implementation\n",
    "Let's start with direct API calls to NVIDIA ~~OpenAI~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35da2d67-7320-4f09-bf49-394e66f4fb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1 is equal to 2. This is a basic arithmetic operation known as addition, where 1 is added to another 1 to produce a sum of 2.\n"
     ]
    }
   ],
   "source": [
    "import requests, base64, os\n",
    "\n",
    "def get_completion(prompt):\n",
    "    invoke_url = \"https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct\"\n",
    "    \n",
    "    headers = {\n",
    "      \"Authorization\": f\"Bearer {os.environ.get('NVIDIA_API_KEY')}\",\n",
    "      \"Accept\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": f'{prompt}'\n",
    "        }\n",
    "      ],\n",
    "      \"max_tokens\": 512,\n",
    "      \"temperature\": 1.00,\n",
    "      \"top_p\": 0.70\n",
    "    }\n",
    "    response = requests.post(invoke_url, headers=headers, json=payload)\n",
    "    return response.json()['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "# response = get_completion(\"What is 1+1?\").json()\n",
    "# print(response['choices'][0]['message']['content'])\n",
    "response = get_completion(\"What is 1+1?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3bc075-4407-4125-8864-2085b6dbd274",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Ollama Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddcc75bb-c3c8-40bf-8f76-99c486987b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's an easy one!\\n\\nThe answer is: 2!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def get_completion_local(prompt):\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "      \"model\": \"llama3\",\n",
    "      \"prompt\":f\"{prompt}\",\n",
    "      \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(invoke_url, json=payload)\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "get_completion_local(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de795e-5fb2-496a-a940-420a958304a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### OpenAI Implementation: Just in Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d4c529e-0097-4064-8df2-32cdd4b33f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21871b67-fd73-4016-8d07-b97875b9580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = XXXXXXXXXXXXXX\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "# llm_model = \"gpt-3.5-turbo\"\n",
    "\n",
    "# def get_completion(prompt, model=llm_model):\n",
    "#     messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#         model=model,\n",
    "#         messages=messages,\n",
    "#         temperature=0, \n",
    "#     )\n",
    "#     return response.choices[0].message[\"content\"]\n",
    "\n",
    "# get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a493d8dd-81ac-40ce-9d7f-878819bc0ee9",
   "metadata": {},
   "source": [
    "### Langchain Abstractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9e18d18-97ee-4320-be5e-be4b83ada2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      "\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b82d1552-7d8e-44ee-936a-38003d18f3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, I am quite frustrated that my blender lid came off and caused a mess in my kitchen. And to add insult to injury, the warranty does not cover the cost of cleaning up my kitchen. I need your help right away, matey!\n"
     ]
    }
   ],
   "source": [
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2d6c545-f220-4e8c-9323-338c08c5ae4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the translated text in a calm and respectful tone using American English:\n",
      "\n",
      "```\n",
      "Gosh, I'm really upset that my blender lid came loose and splattered smoothie all over my kitchen walls! And to make things worse, the warranty doesn't cover the cost of cleaning up the mess. I could really use your help right now.\n",
      "```\n",
      "\n",
      "Let me know if you'd like any further adjustments!\n"
     ]
    }
   ],
   "source": [
    "print(get_completion_local(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95688f21-c7ba-4afa-8093-f5139d07dbb3",
   "metadata": {},
   "source": [
    "## ChatAPI LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a982e-0846-4270-9f5f-25d476fd2cee",
   "metadata": {},
   "source": [
    "### Why Use Prompt Templates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f106d95-45df-4301-8c8b-f2f17ac2a807",
   "metadata": {},
   "source": [
    "![Why Use Prompt Templates](data/images/why-use-prompt-templates.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f810fe-abd1-47de-8cae-69452a543926",
   "metadata": {},
   "source": [
    "![LangChain output parsing works with prompt templates](data/images/output-parsing-works-with-prompts.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e929d4d3-3a2a-4c26-b5c3-9dcbbf4e4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade langchain\n",
    "# !pip install langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4f9f139-3d41-4b11-89b3-a9c401f1d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b1c6cee-363b-4ca4-a448-50b334ec14d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "517fa075-7ef7-46bc-803e-81c97cc91a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3e69cd1-529b-429a-a7f9-7982257f36fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5152d17-a69d-46e7-8913-85ecaef8be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6e21e73-700c-473a-a632-9431b0222a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49322fe6-e212-4608-a61e-60ae08b9f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "295bf1d6-af95-4bcc-9141-26bf8d0693fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\")]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55f810e9-bb43-45f1-b898-4944b9a211aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dbb0f95-dd9a-4e82-ad39-4a85c6954102",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d56be72-1c1f-4912-b2e0-5ca0779654ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa1d05e-822d-40d4-ba00-4ebd77a0f33f",
   "metadata": {},
   "source": [
    "### ChatNVIDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "01d4a890-ba83-447f-aa66-8147d92f4d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "\n",
    "llm_nvidia = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e7632f6-19ee-46e5-97b4-5ec936ad6f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_response = llm_nvidia.invoke(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9947f272-a0f8-4569-97c0-29fc42164364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm really upset that the lid of my blender came off and splattered my kitchen walls with smoothie! To make matters worse, the warranty does not cover the cost of cleaning up my kitchen. I need your help right now, friend!\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "785a8814-361c-4d8e-919d-9ba7704af239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mojo/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ahoy there, me hearty customer! I'm afraid I've got some unfortunate news for ye. The warranty on yer kitchen appliance, specifically the blender, does not cover the costs of cleanin' up after a mishap. It seems ye may have forgotten to put the lid on before startin' the blender, which led to the mess. I'm truly sorry, but them's the rules, I'm afraid. We can't be coverin' every little mishap, as much as we'd like to help. I hope ye understand and have a smoother sailin' in the future. Fare thee well, me friend!\n"
     ]
    }
   ],
   "source": [
    "service_response = llm_nvidia(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6099c15-5149-4d1f-b40c-a5ad424f4e56",
   "metadata": {},
   "source": [
    "### ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caee6ce0-e679-436d-8f63-b235cc88f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm_ollama = ChatOllama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c503e02c-378d-49e8-9524-d3c76371874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the translation:\n",
      "\n",
      "```\n",
      "Ugh, I'm absolutely livid that my blender lid flew off and splattered smoothie all over my kitchen walls! To make things worse, the warranty doesn't cover the cost of cleaning up the mess. I really need some help right now - can you lend a hand? \n",
      "```\n",
      "\n",
      "I've kept the tone calm and respectful, while still conveying your frustration with the situation. I've also used American English spellings (e.g., \"absolutely\" instead of \"be\", \"livid\" instead of \"fuming\", etc.) to ensure the translation is suitable for an American audience.\n"
     ]
    }
   ],
   "source": [
    "customer_response = llm_ollama(customer_messages)\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "189c3d5d-a83c-45ef-8589-4f04e79f61de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, greetings me hearty customer!\n",
      "\n",
      "I be afraid that yer warranty don't cover them fancy cleanin' expenses fer yer galley (that's kitchen speak, matey). Seemingly, ye got yerself into a spot o' trouble by forgettin' to put the lid on yer trusty blender afore startin' it up. Shiver me timbers! A bit o' common sense be in order here, don't ye think?\n",
      "\n",
      "Well, matey, it seems like ye've got yerself a case o' \"Tough Luck\" comin' atcha! But never fear, there be more where that came from! Just keep in mind that next time ye set sail with yer blender, make sure to have the lid securely fastened, or ye might just find yerself walkin' the plank!\n",
      "\n",
      "Fair winds and following seas, me hearty! May yer kitchen be as smooth as a calm sea on a sunny day!\n"
     ]
    }
   ],
   "source": [
    "service_response = llm_ollama(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c499f4-0464-4894-861b-3e7ff1ae7768",
   "metadata": {},
   "source": [
    "### ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "071c3006-cf0c-4031-9fe5-29508e35c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# llm_openai = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3debf5-ba20-46a4-8a5b-76ec26bbb255",
   "metadata": {},
   "source": [
    "## Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f799c41-ce0a-4862-a618-07471af266a1",
   "metadata": {},
   "source": [
    "How we would like the LLM output to look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f233fa4-af9d-4513-b2f1-68ae6d7b285e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2cd0ee7-6d09-411a-b195-968a18910f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e3bfd77-07da-49af-9783-8939d4dca869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfd09fe9-dfe0-4abc-8689-cc962ba3a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96493983-c702-4120-87d9-c517d49b72b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "\"gift\": true,\n",
      "\"delivery_days\": 2,\n",
      "\"price\\_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = llm_nvidia(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fc4d400-6eac-4d3a-aa5e-7ac359c2f719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the extracted information in JSON format:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"gift\": True,\n",
      "    \"delivery_days\": 2,\n",
      "    \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "```\n",
      "\n",
      "Note: The `price_value` key has a list value because there is only one sentence that provides information about the price or value of the product.\n"
     ]
    }
   ],
   "source": [
    "response = llm_ollama(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6d3533d-1c7f-4f88-ab9d-e9341ee7bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = llm_openai(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2df3c42c-a500-4997-a9ab-4b851260c2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f8c04-543d-483a-a325-250050e95210",
   "metadata": {},
   "source": [
    "#### Parse the LLM output string into a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff1f388f-b906-42a5-9779-9afacba9e83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67fb6c8c-1c0e-4920-8077-134b6d2c7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer 'True' if yes,\\\n",
    "                             'False' if not or unknown. Answer should be within quotes\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75b3f558-af1a-4543-8e65-cda22a1e708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4f46d96-e2cf-499d-b86b-77872cdd8f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed573a29-19dd-42ff-9466-7995f0bb6e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer 'True' if yes,                             'False' if not or unknown. Answer should be within quotes\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8874b7e4-cbe8-48b7-87c4-08bb32a5c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information. \n",
    "Don't respond to me back.\n",
    "Just give me the information.\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review, format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a66e1ea7-ba3d-4463-bd21-ad15d8c1cc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='For the following text, extract the following information. \\nDon\\'t respond to me back.\\nJust give me the information.\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\ntext: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife\\'s anniversary present. I think my wife liked it so much she was speechless. So far I\\'ve been the only one using it, and I\\'ve been using it every other morning to clear the leaves on our lawn. It\\'s slightly more expensive than the other leaf blowers out there, but I think it\\'s worth it for the extra features.\\n\\n\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer \\'True\\' if yes,                             \\'False\\' if not or unknown. Answer should be within quotes\\n\\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\\n\\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\\n}\\n```\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c66217b7-7c5d-46f9-a735-3854a22495d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```json\n",
      "{\n",
      "\t\"gift\": \"False\",\n",
      "\t\"delivery_days\": \"2\",\n",
      "\t\"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = llm_nvidia(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4390b72-b46a-4a52-9a62-f5bbcda8beeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gift': 'False', 'delivery_days': '2', 'price_value': \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"}\n"
     ]
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "print(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "babee8ce-44a5-4486-bc21-b43853e1c921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"gift\": \"True\",\n",
      "    \"delivery_days\": \"2\",\n",
      "    \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = llm_ollama(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f2148c7-1af5-4c28-8fe8-94308948cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "baa199f9-7ed7-46eb-b01b-ba1128739691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict['price_value']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
